{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2j1QuAylO9v7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "import requests\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class UrbanLandCoverClassifier:\n",
        "    \"\"\"\n",
        "    Urban Land Cover Classification using multiple ML algorithms\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.data = None\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Load the Urban Land Cover dataset\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # UCI ML Repository URL for Urban Land Cover dataset\n",
        "            url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00295/urban_land_cover.csv\"\n",
        "\n",
        "            print(\"Loading Urban Land Cover dataset...\")\n",
        "            # Try to load from UCI repository\n",
        "            try:\n",
        "                self.data = pd.read_csv(url)\n",
        "            except:\n",
        "                # If direct URL doesn't work, create sample data with similar structure\n",
        "                print(\"Creating sample dataset with similar structure...\")\n",
        "                self.create_sample_data()\n",
        "\n",
        "            print(f\"Dataset loaded successfully! Shape: {self.data.shape}\")\n",
        "            self.display_dataset_info()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data: {e}\")\n",
        "            print(\"Creating sample dataset...\")\n",
        "            self.create_sample_data()\n",
        "\n",
        "    def create_sample_data(self):\n",
        "        \"\"\"\n",
        "        Create a sample dataset similar to Urban Land Cover dataset\n",
        "        Features typically include spectral, spatial, and textural information\n",
        "        \"\"\"\n",
        "        np.random.seed(42)\n",
        "        n_samples = 675  # Similar to original dataset size\n",
        "\n",
        "        # Generate synthetic features similar to urban land cover data\n",
        "        # Spectral features (like satellite imagery bands)\n",
        "        spectral_features = np.random.normal(0, 1, (n_samples, 9))\n",
        "\n",
        "        # Size and shape features\n",
        "        size_shape_features = np.random.exponential(2, (n_samples, 3))\n",
        "\n",
        "        # Texture features\n",
        "        texture_features = np.random.gamma(2, 1, (n_samples, 36))\n",
        "\n",
        "        # Combine all features\n",
        "        features = np.hstack([spectral_features, size_shape_features, texture_features])\n",
        "\n",
        "        # Create feature names\n",
        "        feature_names = (\n",
        "            [f'spectral_{i}' for i in range(9)] +\n",
        "            ['size_feature_1', 'size_feature_2', 'shape_compactness'] +\n",
        "            [f'texture_{i}' for i in range(36)]\n",
        "        )\n",
        "\n",
        "        # Generate target classes (9 classes as in original dataset)\n",
        "        class_names = ['asphalt', 'concrete', 'glass', 'grass', 'painted_metal',\n",
        "                      'red_roof', 'shadow', 'soil', 'tree']\n",
        "\n",
        "        # Create realistic class distribution\n",
        "        class_probs = [0.15, 0.12, 0.08, 0.18, 0.10, 0.12, 0.05, 0.10, 0.10]\n",
        "        y = np.random.choice(class_names, n_samples, p=class_probs)\n",
        "\n",
        "        # Create DataFrame\n",
        "        self.data = pd.DataFrame(features, columns=feature_names)\n",
        "        self.data['class'] = y\n",
        "\n",
        "        print(\"Sample dataset created with 48 features and 9 classes\")\n",
        "\n",
        "    def display_dataset_info(self):\n",
        "        \"\"\"\n",
        "        Display information about the dataset\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"DATASET INFORMATION\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Dataset shape: {self.data.shape}\")\n",
        "        print(f\"Features: {self.data.shape[1] - 1}\")\n",
        "        print(f\"Samples: {self.data.shape[0]}\")\n",
        "\n",
        "        if 'class' in self.data.columns:\n",
        "            print(f\"\\nClass distribution:\")\n",
        "            print(self.data['class'].value_counts().sort_index())\n",
        "\n",
        "        print(f\"\\nFirst few rows:\")\n",
        "        print(self.data.head())\n",
        "\n",
        "        print(f\"\\nDataset info:\")\n",
        "        print(self.data.info())\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        \"\"\"\n",
        "        Preprocess the data: handle missing values, encode labels, split data\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"DATA PREPROCESSING\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Separate features and target\n",
        "        if 'class' in self.data.columns:\n",
        "            self.X = self.data.drop('class', axis=1)\n",
        "            self.y = self.data['class']\n",
        "        else:\n",
        "            # Assume last column is target\n",
        "            self.X = self.data.iloc[:, :-1]\n",
        "            self.y = self.data.iloc[:, -1]\n",
        "\n",
        "        # Handle missing values\n",
        "        missing_values = self.X.isnull().sum().sum()\n",
        "        if missing_values > 0:\n",
        "            print(f\"Handling {missing_values} missing values...\")\n",
        "            self.X = self.X.fillna(self.X.mean())\n",
        "\n",
        "        # Encode labels\n",
        "        self.y_encoded = self.label_encoder.fit_transform(self.y)\n",
        "\n",
        "        # Split the data\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.X, self.y_encoded, test_size=0.2, random_state=42,\n",
        "            stratify=self.y_encoded\n",
        "        )\n",
        "\n",
        "        # Scale the features\n",
        "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
        "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
        "\n",
        "        print(f\"Training set size: {self.X_train.shape}\")\n",
        "        print(f\"Test set size: {self.X_test.shape}\")\n",
        "        print(f\"Number of classes: {len(np.unique(self.y_encoded))}\")\n",
        "        print(\"Data preprocessing completed!\")\n",
        "\n",
        "    def perform_eda(self):\n",
        "        \"\"\"\n",
        "        Perform Exploratory Data Analysis\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"EXPLORATORY DATA ANALYSIS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "\n",
        "        # Class distribution\n",
        "        plt.subplot(2, 3, 1)\n",
        "        self.y.value_counts().plot(kind='bar')\n",
        "        plt.title('Class Distribution')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # Feature correlation heatmap (subset of features)\n",
        "        plt.subplot(2, 3, 2)\n",
        "        # Select first 10 features for visualization\n",
        "        corr_matrix = self.X.iloc[:, :10].corr()\n",
        "        sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\n",
        "        plt.title('Feature Correlation (First 10 Features)')\n",
        "\n",
        "        # Feature distributions (first few features)\n",
        "        for i in range(4):\n",
        "            plt.subplot(2, 3, i+3)\n",
        "            self.X.iloc[:, i].hist(bins=20)\n",
        "            plt.title(f'Distribution: {self.X.columns[i]}')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # PCA Analysis\n",
        "        pca = PCA()\n",
        "        X_pca = pca.fit_transform(self.X_train_scaled)\n",
        "\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "        plt.xlabel('Number of Components')\n",
        "        plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "        plt.title('PCA - Explained Variance')\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.scatter(X_pca[:, 0], X_pca[:, 1], c=self.y_train, alpha=0.6)\n",
        "        plt.xlabel('First Principal Component')\n",
        "        plt.ylabel('Second Principal Component')\n",
        "        plt.title('PCA - First Two Components')\n",
        "        plt.colorbar()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def train_models(self):\n",
        "        \"\"\"\n",
        "        Train multiple machine learning models\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"MODEL TRAINING\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Define models\n",
        "        self.models = {\n",
        "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "            'SVM': SVC(kernel='rbf', random_state=42),\n",
        "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "            'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
        "            'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42, max_iter=500)\n",
        "        }\n",
        "\n",
        "        # Train and evaluate each model\n",
        "        for name, model in self.models.items():\n",
        "            print(f\"\\nTraining {name}...\")\n",
        "\n",
        "            # Train the model\n",
        "            model.fit(self.X_train_scaled, self.y_train)\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = model.predict(self.X_test_scaled)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            accuracy = accuracy_score(self.y_test, y_pred)\n",
        "\n",
        "            # Cross-validation\n",
        "            cv_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=5)\n",
        "\n",
        "            # Store results\n",
        "            self.results[name] = {\n",
        "                'model': model,\n",
        "                'accuracy': accuracy,\n",
        "                'cv_mean': cv_scores.mean(),\n",
        "                'cv_std': cv_scores.std(),\n",
        "                'predictions': y_pred\n",
        "            }\n",
        "\n",
        "            print(f\"Accuracy: {accuracy:.4f}\")\n",
        "            print(f\"CV Mean: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "\n",
        "    def hyperparameter_tuning(self):\n",
        "        \"\"\"\n",
        "        Perform hyperparameter tuning for the best model\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"HYPERPARAMETER TUNING\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Find the best performing model\n",
        "        best_model_name = max(self.results.keys(), key=lambda k: self.results[k]['accuracy'])\n",
        "        print(f\"Best model: {best_model_name}\")\n",
        "\n",
        "        # Define hyperparameters for Random Forest (as an example)\n",
        "        if best_model_name == 'Random Forest':\n",
        "            param_grid = {\n",
        "                'n_estimators': [50, 100, 200],\n",
        "                'max_depth': [None, 10, 20],\n",
        "                'min_samples_split': [2, 5, 10]\n",
        "            }\n",
        "\n",
        "            grid_search = GridSearchCV(\n",
        "                RandomForestClassifier(random_state=42),\n",
        "                param_grid, cv=5, scoring='accuracy', n_jobs=-1\n",
        "            )\n",
        "\n",
        "            print(\"Performing Grid Search...\")\n",
        "            grid_search.fit(self.X_train_scaled, self.y_train)\n",
        "\n",
        "            print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "            print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "            # Update the best model\n",
        "            self.results['Best Tuned Model'] = {\n",
        "                'model': grid_search.best_estimator_,\n",
        "                'accuracy': accuracy_score(self.y_test, grid_search.predict(self.X_test_scaled)),\n",
        "                'predictions': grid_search.predict(self.X_test_scaled)\n",
        "            }"
      ]
    }
  ]
}